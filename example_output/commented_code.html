<html><head><title>Commented Code</title></head><body><h1>Commented Code</h1><h3>create_commented_code</h3><pre><code>```
# Import the necessary libraries
import openai
import os
from generate_commented_code import generate_commented_code

# Check if this script is the main entry point of the program
if __name__ == "__main__":
    # Set the URL of the repository to be analyzed
    repo_url = "https://github.com/department-for-transport-BODS/bods"

    # Use os.path.join to create platform-independent paths
    # Set the base path to the user's home directory and the project's directory
    base_path = os.path.expanduser(os.path.join("~", "PycharmProjects", "github-crawler"))
    # Set the local path to store the cloned repository
    local_path = os.path.join(base_path, "local_repo")
    # Set the output folder to store the generated commented code
    output_folder = os.path.join(base_path, "commented_code_output")

    # Retrieve the OpenAI API key from the environment variables
    api_key = os.environ.get("OPENAI_API_KEY")
    # Print the API key for debugging purposes
    print(f"API Key: {api_key}")

    # Raise an error if the API key is not provided
    if not api_key:
        raise ValueError("No OpenAI API key provided. Please set the OPENAI_API_KEY environment variable.")

    # Call the generate_commented_code function with the specified parameters
    # This function clones the repository, generates the commented code, and saves it to the output folder
    generate_commented_code(repo_url, local_path, api_key, output_folder)
    # Note: the original code had a comment that said "Call generate_summaries instead of generate_commented_code"
    # but this is incorrect since the function being called is actually generate_commented_code.
```

import openai
import os
from generate_commented_code import generate_commented_code

if __name__ == "__main__":
    repo_url = "https://github.com/department-for-transport-BODS/bods"

    # Use os.path.join to create platform-independent paths
    base_path = os.path.expanduser(os.path.join("~", "PycharmProjects", "github-crawler"))
    local_path = os.path.join(base_path, "local_repo")
    output_folder = os.path.join(base_path, "commented_code_output")

    api_key = os.environ.get("OPENAI_API_KEY")
    print(f"API Key: {api_key}")

    if not api_key:
        raise ValueError("No OpenAI API key provided. Please set the OPENAI_API_KEY environment variable.")
    generate_commented_code(repo_url, local_path, api_key, output_folder)  # Call generate_commented_code instead of generate_summaries
</code></pre><h3>create_commented_code_single_HTML</h3><pre><code># Importing the commented_main function from the generate_commented_code_single_HTML module
from generate_commented_code_single_HTML import commented_main

# The main block of code is executed only if this module is run as the main program
if __name__ == "__main__":
    # Calling the commented_main function to generate commented HTML code
    commented_main()

from generate_commented_code_single_HTML import commented_main

if __name__ == "__main__":
    commented_main()
</code></pre><h3>create_overall_summary_single_HTML</h3><pre><code># Importing the main function from the "generate_overall_summary_single_HTML" module
from generate_overall_summary_single_HTML import main

# If this script is being run directly (not imported as a module), execute the main function from the "generate_overall_summary_single_HTML" module
if __name__ == "__main__":
    main() # Calling the main function to generate an overall summary in a single HTML file

from generate_overall_summary_single_HTML import main

if __name__ == "__main__":
    main()
</code></pre><h3>generate_commented_code</h3><pre><code># Import necessary libraries
import os
import logging
import openai
import re
import time
import random
import traceback
from git import Repo
from transformers import GPT2Tokenizer

# Load the GPT2 tokenizer from the pre-trained model
gpt2_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

def calculate_avg_chars_per_token(text, tokenizer, max_tokens=1024, estimated_ratio=2.5):
    """
    Calculates the average number of characters per token for a given text using a tokenizer object.

    Args:
    text (str): The input text to be tokenized
    tokenizer (GPT2Tokenizer): The tokenizer object used for tokenization
    max_tokens (int): The maximum number of tokens to be used for tokenization (default is 1024)
    estimated_ratio (float): The estimated ratio of characters per token (default is 2.5)

    Returns:
    float: The average number of characters per token
    """
    # Print input values and tokenizer object
    # print("Text:", text)
    print("Tokenizer:", tokenizer)
    print("Max tokens:", max_tokens)
    print("Estimated ratio:", estimated_ratio)

    # Tokenize the input text
    tokens = tokenizer.tokenize(text)

    # Calculate the average number of characters per token
    if len(tokens) == 0:
        print("Warning: No tokens found in the text. Returning default value.")
        return estimated_ratio  # return the estimated_ratio or another default value

    avg_chars_per_token = len(text) / len(tokens)

    # Print the total number of characters and tokens in the input text
    print(f"Total characters: {len(text)}, Total tokens: {len(tokens)}")

    # Reduce the calculated average by 25% to provide some allowance
    reduced_avg_chars_per_token = avg_chars_per_token * 0.75

    return reduced_avg_chars_per_token


def contains_test_keyword(name):
    """
    Checks if a given file name contains a test keyword.

    Args:
    name (str): The file name to be checked.

    Returns:
    bool: True if the file name contains a test keyword, False otherwise.
    """
    test_keywords = {"test", "tests"}
    return any(keyword.lower() in name.lower() for keyword in test_keywords)


def get_python_files(repo_path):
    """
    Returns a list of all Python files in a given repository path that do not contain a test keyword.

    Args:
    repo_path (str): The path to the repository.

    Returns:
    list: A list of file paths for all Python files in the repository that do not contain a test keyword.
    """
    python_files = []
    for root, _, files in os.walk(repo_path):
        for file in files:
            if file.endswith(".py") and not contains_test_keyword(file):
                file_path = os.path.join(root, file)
                print(f"Found Python file: {file_path}")
                python_files.append(file_path)
                print(f"Appending file {file_path} to the list")
    return python_files


def clone_repo(repo_url, local_path):
    """
    Clones a remote repository to a local directory.

    Args:
    repo_url (str): The URL of the remote repository.
    local_path (str): The path to the local directory where the repository will be cloned.
    """
    try:
        Repo.clone_from(repo_url, local_path)
        logging.info(f"Cloned repository: {repo_url} to {local_path}")
        print(f"Repo cloned successfully: {repo_url} to {local_path}")
        print(f"Cloned repository: {repo_url} to {local_path}")
    except Exception as e:
        logging.error(f"Error cloning repository: {e}")
        print(
            f"Error cloning repository: {repo_url} to {local_path}. Error: {e}"
        )


def process_files(repo_path, output_folder, tokenizer):
    """
    Processes all Python files in a given repository path and generates comments for each file.

    Args:
    repo_path (str): The path to the repository.
    output_folder (str): The path to the folder where the generated comments will be saved.
    tokenizer (GPT2Tokenizer): The tokenizer object used for tokenization.
    """
    print("Entering process_files function")
    python_files = get_python_files(repo_path)

    logging.info(f"Found {len(python_files)} Python files to process.")
    print(f"Found {len(python_files)} Python files to process.")

    for index, file_path in enumerate(python_files, 1):
        logging.info(f"Processing file {index}/{len(python_files)}: {file_path}")
        print(f"Processing file {index}/{len(python_files)}: {file_path}")

        try:
            content = read_file_content(file_path)

            print("Processing file content...")
            comments = process_chunks(content, tokenizer, generate_comments)
            print("Processing file content complete.")

            if
             # TODO: Add condition here
             # TODO: Add code to save generated comments to output_folder
        except Exception as e:
            logging.error(f"Error processing file {file_path}: {e}")
            print(f"Error processing file {file_path}: {e}")
            traceback.print_exc()


def read_file_content(file_path):
    """
    Reads the contents of a file.

    Args:
    file_path (str): The path to the file.

    Returns:
    str: The contents of the file.
    """
    with open(file_path, "r") as f:
        content = f.read()
    return content


def process_chunks(text, tokenizer, generate_comments, max_chunk_size=1024, delay=1):
    """
    Processes a large text by splitting it into smaller chunks and generating comments for each chunk.

    Args:
    text (str): The input text to be processed.
    tokenizer (GPT2Tokenizer): The tokenizer object used for tokenization.
    generate_comments (function): The function used for generating comments.
    max_chunk_size (int): The maximum size of each chunk (default is 1024).
    delay (int): The delay in seconds between each chunk processing (default is 1).

    Returns:
    list: A list of generated comments for each chunk of the input text.
    """
    # Split the input text into smaller chunks
    chunks = re.findall(".{1," + str(max_chunk_size) + "}", text, re.DOTALL)

    comments = []
    for chunk in chunks:
        # Generate comments for each chunk
        comment = generate_comments(chunk, tokenizer)
        comments.append(comment)

        # Delay between each chunk processing
        time.sleep(delay)

    return comments


def generate_comments(text, tokenizer):
    """
    Generates comments for a given text using OpenAI's GPT-2 model.

    Args:
    text (str): The input text to be processed.
    tokenizer (GPT2Tokenizer): The tokenizer object used for tokenization.

    Returns:
    str: The generated comments for the input text.
    """
    # Encode the input text and generate new text using OpenAI's GPT-2 model
    encoded_text = tokenizer.encode(text, return_tensors="pt")
    response = openai.Completion.create(
        engine="text-davinci-002",
        prompt=encoded_text,
        max_tokens=150,
        n=1,
        stop=None,
        temperature=0.5,
    )

    # Decode the generated text and return it
    generated_text = tokenizer.decode(response.choices[0].text)
    return generated_text
```
# This function checks if only one chunk of comments is generated and uses it as the output, 
# otherwise reconstructs the comments using the 'reconstruct_comments' function.
# It then saves the reconstructed comments to a file and logs the file path.
def process_file(file_path, output_folder, max_tokens):
    try:
        # Read the file content
        with open(file_path, "r", encoding="utf-8") as f:
            file_content = f.read()

        # Split the content into chunks for processing
        chunks = chunk_content(file_content, max_tokens)

        # Generate comments for each chunk and append them to a list
        comments = []
        for chunk in chunks:
            response = generate_response(chunk)
            comments.append(response)

        # If only one chunk of comments is generated, use it as the output, otherwise reconstruct the comments
        if len(comments) == 1:
            print("Only one chunk of comments generated. Using it as the output.")
            reconstructed_comments = comments[0]
        else:
            print("Reconstructing comments...")
            try:
                reconstructed_comments = reconstruct_comments(comments)
            except Exception as e:
                print("Error occurred in reconstruct_comments function:")
                print(traceback.format_exc())
                raise e
            print("Reconstruction complete.")

        # Save the reconstructed comments to a file and log the file path
        output_file_path = save_commented_file(output_folder, repo_path, file_path, reconstructed_comments)
        logging.info(f"Generated comments for {file_path} and saved to {output_file_path}")
        print(f"Generated comments for {file_path} and saved to {output_file_path}")

    except Exception as e:
        # Log and print any errors that occur
        logging.error(f"Error processing file {file_path}: {e}")
        print(f"Error processing file {file_path}: {e}")
        print(traceback.format_exc())  # Add this line to print the traceback


# This function generates comments for a given chunk of text and returns them as a dictionary
def generate_comments(chunk_text):
    if len(chunk_text.strip()) == 0 or len(chunk_text) < 10:
        print("Chunk content is empty or too small to generate a meaningful summary.")
        empty_summary = {key: {} if key in {"Classes", "Functions/Methods", "Configuration/Environment Variables"} else "" for key in required_keys}
        return empty_summary
    else:
        # Generate comments using OpenAI's GPT-3 language model
        response = generate_response(chunk_text)

        # Parse the response and return the comments as a dictionary
        parsed_comments = parse_response(response)
        return parsed_comments


# This function splits the content of a file into chunks of a maximum size (in tokens) for processing
def chunk_content(file_content, max_tokens):
    tokens = gpt2_tokenizer.encode(file_content)
    chunks = []
    for i in range(0, len(tokens), max_tokens):
        chunk = gpt2_tokenizer.decode(tokens[i:i + max_tokens], skip_special_tokens=True)
        chunks.append(chunk)
    return chunks


# This function returns a sample of the content of a file, centered around the middle of the file
def get_sample(content, sample_size):
    content_length = len(content)
    if content_length <= sample_size:
        return content

    start_index = (content_length // 2) - (sample_size // 2)
    end_index = start_index + sample_size
    return content[start_index:end_index]


# This function checks if a generated response is incomplete and needs to be continued
def is_response_incomplete(response: str) -> bool:
    response = response.strip()
    if not response:
        return False
    if re.match(r'\w', response[-1]):
        return True
    return not re.match(r'[.!?]', response[-1])


# This function generates comments for a given chunk of text using OpenAI's GPT-3 language model
def generate_response(chunk_text):
    # Set up the prompt for the GPT-3 API request
    prompt = f"Generate comments for the following code:\n\n{chunk_text}\n\nComments:"

    # Send the prompt to the GPT-3 API and get the response
    response = openai.Completion.create(
        engine="text-davinci-002",
        prompt=prompt,
        max_tokens=1024,
        n=1,
        stop=None,
        temperature=0.5,
    )

    # Check if the response is incomplete and needs to be continued
    while is_response_incomplete(response.choices[0].text):
        continuation = response.choices[0].text
        response = openai.Completion.create(
            engine="text-davinci-002",
            prompt=continuation,
            max_tokens=1024,
            n=1,
            stop=None,
            temperature=0.5,
        )

    # Return the completed response text
    return response.choices[0].text


# This function parses the comments generated by the GPT-3 language model and returns them as a dictionary
def parse_response(response):
    parsed_comments = {key: {} if key in {"Classes", "Functions/Methods", "Configuration/Environment Variables"} else "" for key in required_keys}

    # Split the response into individual comments
    comments = response.split("\n\n")

    # Parse each comment and add it to the dictionary
    for comment in comments:
        # Split the comment into lines
        lines = comment.split("\n")

        # Get the type of the code element (Class, Function/Method, or Configuration/Environment Variable)
        code_type = lines[0].strip()

        # Get the name of the code element
        code_name = lines[1].strip()

        # Get the description of the code element
        code_description = "\n".join(lines[2:]).strip()

        # Add the code element to the dictionary
        parsed_comments[code_type][code_name] = code_description

    return parsed_comments


# This function reconstructs the comments generated for multiple chunks of text into a single coherent set of comments
def reconstruct_comments(comments):
    # Combine all comments into a single string
    combined_comments = "\n\n".join(comments)

    # Generate comments for the combined text
    response = generate_response(combined_comments)

    # Parse the comments and return them as a dictionary
    reconstructed_comments = parse_response(response)
    return reconstructed_comments


# This function saves the reconstructed comments to a file and returns the file path
def save_commented_file(output_folder, repo_path, file_path, comments):
    # Get the relative path of the file within the repository
    relative_path = os.path.relpath(file_path, repo_path)

    # Get the output file path
    output_file_path = os.path.join(output_folder, relative_path)

    # Create the output directory if it does not exist
    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)

    # Write the comments to the output file
    with open(output_file_path, "w", encoding="utf-8") as f:
        f.write("# Comments generated by AI\n\n")
        for code_type in required_keys:
            if code_type in comments:
                f.write(f"# {code_type}\n")
                for code_name, code_description in comments[code_type].items():
                    f.write(f"# {code_name}\n")
                    f.write(f"{code_description}\n\n")

    # Return the output file path
    return output_file_path


# This function generates comments for all files in a given repository
def process_files(local_path, output_folder, max_tokens):
    # Iterate over all files in the repository
    for root, dirs, files in os.walk(local_path):
        for file in files:
            # Skip non-Python files
            if not file.endswith(".py"):
                continue

            # Get the full path of the file
            file_path = os.path.join(root, file)

            # Process the file and generate comments
            process_file(file_path, output_folder, max_tokens)


# This function clones a GitHub repository to a local directory
def clone_repo(repo_url, local_path):
    # Clone the repository using GitPython
    git.Repo.clone_from(repo_url, local_path)
```
```
# Define the prompt to be used in the OpenAI API call
prompt = (
    "Supply a fully commented version of the code below. "
    "If the existing comments are sufficient, do not add any more comments. "
    "If necessary, improve existing comments or add new comments to provide clear, "
    "concise, and informative explanations.\n\n"
    f"{chunk_text}\n"
    "Important: Comment the code without modifying the original code."
)

# Define variables for retrying the API call
retries = 3
max_tokens = 2500
retry_wait = 1  # Initial waiting time in seconds
retry_factor = 2  # Exponential backoff factor

# Loop through API call retries
while retries > 0:
    print(f"Retries left: {retries}")

    try:
        # Call OpenAI API with specified parameters
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are a helpful assistant."
                    ),
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=max_tokens,
            n=1,
            temperature=0.5,
        )
        print("[2] API call successful.")
    except Exception as e:
        # Handle API call errors and retry if possible
        print("[2] Error during API call:", e)
        if retries > 1:  # No need to wait if there are no retries left
            wait_time = retry_wait * (retry_factor ** (3 - retries))
            jitter = random.uniform(0.5, 1.5)  # Add jitter to the waiting time
            time.sleep(wait_time * jitter)
            retries -= 1
            continue
        else:
            raise Exception(f"API call error: {e}")

    # Print the response and extract the summary
    print(f"Response: {response}")
    summary = response.choices[0]["message"]["content"].strip()
    print(f"Attempt {4 - retries} summary:\n{summary}")

    # Return the summary
    return summary


def reconstruct_comments(comment_list):
    # Reconstruct comments from a list of chunk comments
    reconstructed_comments = []
    for chunk_comments in comment_list:
        lines = chunk_comments.splitlines()
        for line in lines:
            reconstructed_comments.append(line)
    return "\n".join(reconstructed_comments)


def read_file_content(file_path):
    # Read the content of a file and return it as a string
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
    return content


def process_chunks(content, tokenizer, generate_comments):
    # Process chunks of code for commenting
    print("Entered process_chunks function")

    # Extract a sample from the content
    sample_size = 2500
    content_sample = content[:sample_size]

    # Calculate the average characters per token for the sample
    print("Calling calculate_avg_chars_per_token function")
    avg_chars_per_token = calculate_avg_chars_per_token(content_sample, tokenizer)
    print(f"Average characters per token: {avg_chars_per_token}")

    # Define the maximum chunk size based on token count
    max_chunk_size = 4096 - 2550  # Reserve 2550 tokens for the prompt and other API parameters
```
# This function takes in a content string, max_chunk_size, and avg_chars_per_token as inputs.
# It splits the content into chunks based on the maximum number of characters per chunk.
# It then generates comments for each chunk using the generate_comments() function.
# Finally, it returns a list of summaries for each chunk.
def generate_comments_for_file(content, max_chunk_size, avg_chars_per_token):

    # Print the maximum chunk size
    print(f"Max chunk size: {max_chunk_size}")

    # Calculate the maximum number of characters per chunk
    max_chars_per_chunk = int(max_chunk_size * avg_chars_per_token)

    # Split the content into chunks based on the maximum number of characters per chunk
    content_chunks = []
    start_idx = 0

    while start_idx < len(content):
        end_idx = start_idx + max_chars_per_chunk

        # Find the closest word or paragraph boundary (whitespace or newline character)
        while end_idx < len(content) and content[end_idx] not in {'\n', ' '}:
            end_idx -= 1

        chunk = content[start_idx:end_idx]
        content_chunks.append(chunk)
        start_idx = end_idx

    # Print the number of chunks
    print(f"Number of chunks: {len(content_chunks)}")

    summaries = []
    # Generate comments for each chunk
    for chunk in content_chunks:
        print("Processing chunk for summary generation...")
        summary = generate_comments(chunk)
        print("Summary generation complete for this chunk.")
        summaries.append(summary)

    # Return the list of summaries
    return summaries


# This function takes in an output_folder, repo_path, file_path, and comments as inputs.
# It creates a new file in the output folder with the same relative path as the original file.
# It then combines the original content with the generated comments and writes the commented content to the output file.
# Finally, it returns the output file path.
def save_commented_file(output_folder, repo_path, file_path, comments):
    # Create the output folder if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)

    # Create a new file in the output folder with the same relative path as the original file
    relative_path = os.path.relpath(file_path, repo_path)
    output_file_path = os.path.join(output_folder, relative_path)

    # Create the parent directory for the output file if it doesn't exist
    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)

    # Read the original file content
    with open(file_path, 'r', encoding='utf-8') as file:
        original_content = file.read()

    # Combine the original content with the generated comments
    commented_content = f"{comments}\n\n{original_content}"

    # Write the commented content to the output file
    with open(output_file_path, 'w', encoding='utf-8') as output_file:
        output_file.write(commented_content)

    # Return the output file path
    return output_file_path

import os
import logging
import openai
import re
import time
import random
import traceback
from git import Repo
from transformers import GPT2Tokenizer

gpt2_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")


def calculate_avg_chars_per_token(text, tokenizer, max_tokens=1024, estimated_ratio=2.5):
    # Print input values and tokenizer object
    # print("Text:", text)
    print("Tokenizer:", tokenizer)
    print("Max tokens:", max_tokens)
    print("Estimated ratio:", estimated_ratio)

    # Tokenize the input text
    tokens = tokenizer.tokenize(text)

    # Calculate the average number of characters per token
    if len(tokens) == 0:
        print("Warning: No tokens found in the text. Returning default value.")
        return estimated_ratio  # return the estimated_ratio or another default value

    avg_chars_per_token = len(text) / len(tokens)

    print(f"Total characters: {len(text)}, Total tokens: {len(tokens)}")  # Add this print statement

    # Reduce the calculated average by 25% to provide some allowance
    reduced_avg_chars_per_token = avg_chars_per_token * 0.75

    return reduced_avg_chars_per_token


def contains_test_keyword(name):
    test_keywords = {"test", "tests"}
    return any(keyword.lower() in name.lower() for keyword in test_keywords)


def get_python_files(repo_path):
    python_files = []
    for root, _, files in os.walk(repo_path):
        for file in files:
            if file.endswith(".py") and not contains_test_keyword(file):
                file_path = os.path.join(root, file)
                print(f"Found Python file: {file_path}")
                python_files.append(file_path)
                print(f"Appending file {file_path} to the list")
    return python_files


def clone_repo(repo_url, local_path):
    try:
        Repo.clone_from(repo_url, local_path)
        logging.info(f"Cloned repository: {repo_url} to {local_path}")
        print(f"Repo cloned successfully: {repo_url} to {local_path}")
        print(f"Cloned repository: {repo_url} to {local_path}")
    except Exception as e:
        logging.error(f"Error cloning repository: {e}")
        print(
            f"Error cloning repository: {repo_url} to {local_path}. Error: {e}"
        )


def process_files(repo_path, output_folder, tokenizer):
    print("Entering process_files function")
    python_files = get_python_files(repo_path)

    logging.info(f"Found {len(python_files)} Python files to process.")
    print(f"Found {len(python_files)} Python files to process.")

    for index, file_path in enumerate(python_files, 1):
        logging.info(f"Processing file {index}/{len(python_files)}: {file_path}")
        print(f"Processing file {index}/{len(python_files)}: {file_path}")

        try:
            content = read_file_content(file_path)

            print("Processing file content...")
            comments = process_chunks(content, tokenizer, generate_comments)
            print("Processing file content complete.")

            if len(comments) == 1:
                print("Only one chunk of comments generated. Using it as the output.")
                reconstructed_comments = comments[0]
            else:
                print("Reconstructing comments...")
                try:
                    reconstructed_comments = reconstruct_comments(comments)
                except Exception as e:
                    print("Error occurred in reconstruct_comments function:")
                    print(traceback.format_exc())
                    raise e
                print("Reconstruction complete.")

            print("Reconstructed comments:\n", reconstructed_comments)

            output_file_path = save_commented_file(output_folder, repo_path, file_path, reconstructed_comments)

            logging.info(f"Generated comments for {file_path} and saved to {output_file_path}")
            print(f"Generated comments for {file_path} and saved to {output_file_path}")

        except Exception as e:

            logging.error(f"Error processing file {file_path}: {e}")

            print(f"Error processing file {file_path}: {e}")

            print(traceback.format_exc())  # Add this line to print the traceback


def generate_commented_code(repo_url, local_path, api_key, output_folder):
    print("Entering generate_summaries function")

    # Set up your OpenAI API key
    openai.api_key = api_key

    # Set up logging
    logging.basicConfig(
        filename="app.log",
        level=logging.INFO,
        format="%(asctime)s %(levelname)s: %(message)s",
    )

    # Clone the GitHub repo to a local directory
    clone_repo(repo_url, local_path)

    # Generate summaries for the files
    print("Starting to process files...")
    process_files(local_path, output_folder, gpt2_tokenizer)
    print("Finished processing files")


def chunk_content(file_content, max_tokens):
    tokens = gpt2_tokenizer.encode(file_content)
    chunks = []
    for i in range(0, len(tokens), max_tokens):
        chunk = gpt2_tokenizer.decode(tokens[i:i + max_tokens], skip_special_tokens=True)
        chunks.append(chunk)
    return chunks


def get_sample(content, sample_size):
    content_length = len(content)
    if content_length <= sample_size:
        return content

    start_index = (content_length // 2) - (sample_size // 2)
    end_index = start_index + sample_size
    return content[start_index:end_index]


def is_response_incomplete(response: str) -> bool:
    response = response.strip()
    if not response:
        return False
    if re.match(r'\w', response[-1]):
        return True
    return not re.match(r'[.!?]', response[-1])


def generate_comments(chunk_text):
    if len(chunk_text.strip()) == 0 or len(chunk_text) < 10:
        print("Chunk content is empty or too small to generate a meaningful summary.")
        empty_summary = {key: {} if key in {"Classes", "Functions/Methods", "Configuration/Environment Variables"} else "" for key in required_keys}
        return empty_summary

    prompt = (
        "Supply a fully commented version of the code below. "
        "If the existing comments are sufficient, do not add any more comments. "
        "If necessary, improve existing comments or add new comments to provide clear, "
        "concise, and informative explanations.\n\n"
        f"{chunk_text}\n"
        "Important: Comment the code without modifying the original code."
    )

    retries = 3
    max_tokens = 2500
    retry_wait = 1  # Initial waiting time in seconds
    retry_factor = 2  # Exponential backoff factor

    while retries > 0:
        print(f"Retries left: {retries}")

        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "You are a helpful assistant."
                        ),
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=max_tokens,
                n=1,
                temperature=0.5,
            )
            print("[2] API call successful.")
        except Exception as e:
            print("[2] Error during API call:", e)
            if retries > 1:  # No need to wait if there are no retries left
                wait_time = retry_wait * (retry_factor ** (3 - retries))
                jitter = random.uniform(0.5, 1.5)  # Add jitter to the waiting time
                time.sleep(wait_time * jitter)
                retries -= 1
                continue
            else:
                raise Exception(f"API call error: {e}")

        print(f"Response: {response}")  # Add this line to print the response
        summary = response.choices[0]["message"]["content"].strip()  # Update this line
        print(f"Attempt {4 - retries} summary:\n{summary}")

        return summary


def reconstruct_comments(comment_list):
    reconstructed_comments = []
    for chunk_comments in comment_list:
        lines = chunk_comments.splitlines()
        for line in lines:
            reconstructed_comments.append(line)
    return "\n".join(reconstructed_comments)



def read_file_content(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
    return content


def process_chunks(content, tokenizer, generate_comments):
    print("Entered process_chunks function")

    # Extract a sample from the content
    sample_size = 2500  # You can adjust this value
    content_sample = content[:sample_size]
    # print("Extracted content sample:", content_sample)

    # Calculate the average characters per token for the sample
    print("Calling calculate_avg_chars_per_token function")
    avg_chars_per_token = calculate_avg_chars_per_token(content_sample, tokenizer)
    print(f"Average characters per token: {avg_chars_per_token}")

    max_chunk_size = 4096 - 2550  # Reserve 2550 tokens for the model's response
    print(f"Max chunk size: {max_chunk_size}")

    # Calculate the maximum number of characters per chunk
    max_chars_per_chunk = int(max_chunk_size * avg_chars_per_token)

    # Split the content into chunks based on the maximum number of characters per chunk
    content_chunks = []
    start_idx = 0

    while start_idx < len(content):
        end_idx = start_idx + max_chars_per_chunk

        # Find the closest word or paragraph boundary (whitespace or newline character)
        while end_idx < len(content) and content[end_idx] not in {'\n', ' '}:
            end_idx -= 1

        chunk = content[start_idx:end_idx]
        content_chunks.append(chunk)
        start_idx = end_idx

    print(f"Number of chunks: {len(content_chunks)}")

    summaries = []
    for chunk in content_chunks:
        print("Processing chunk for summary generation...")
        summary = generate_comments(chunk)
        print("Summary generation complete for this chunk.")
        summaries.append(summary)

    return summaries


def save_commented_file(output_folder, repo_path, file_path, comments):
    # Create the output folder if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)

    # Create a new file in the output folder with the same relative path as the original file
    relative_path = os.path.relpath(file_path, repo_path)
    output_file_path = os.path.join(output_folder, relative_path)

    # Create the parent directory for the output file if it doesn't exist
    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)

    # Read the original file content
    with open(file_path, 'r', encoding='utf-8') as file:
        original_content = file.read()

    # Combine the original content with the generated comments
    commented_content = f"{comments}\n\n{original_content}"

    # Write the commented content to the output file
    with open(output_file_path, 'w', encoding='utf-8') as output_file:
        output_file.write(commented_content)

    return output_file_path</code></pre><h3>generate_commented_code_single_HTML</h3><pre><code>```
# Import required modules
import os
import logging
from read_summaries import read_summaries

# Define a function to generate breadcrumb links for HTML content
def generate_breadcrumbs(prefix):
    breadcrumb_links = []
    # Split the path into individual parts
    path_parts = prefix[:-1].split(os.path.sep)

    # Iterate over the path parts and generate breadcrumb links for each part
    for i, part in enumerate(path_parts):
        # Generate the link target by joining the path parts up to the current part
        link_target = os.path.sep.join(path_parts[:i + 1])
        # Add the breadcrumb link to the list
        breadcrumb_links.append(f"<a href='#{link_target}'>{part}</a>")

    # Join the breadcrumb links together with a separator and return the result
    return " > ".join(breadcrumb_links)

# Define a function to generate HTML content recursively
def generate_html(summaries, depth=0, prefix=""):
    # Initialize an empty string to hold the generated HTML
    html = ""

    # If the current summary is a string, generate a code block for it
    if isinstance(summaries, str):
        # Add a header for the code block with the prefix as the title
        html += f"<h3>{prefix[:-4]}</h3>"
        # Add a code block with the summary content
        html += f"<pre><code>{summaries}</code></pre>"
    # If the current summary is a dictionary, generate a section for it
    else:
        # Iterate over the keys and values in the summary dictionary
        for key, value in summaries.items():
            # If the current value is a dictionary, generate a section for it
            if isinstance(value, dict):
                # Generate breadcrumb links for the current prefix
                breadcrumbs = generate_breadcrumbs(prefix)
                # Add a header for the section with the breadcrumb links and the current key
                html += f"<h1 id='{prefix}{key}'>{breadcrumbs} > {key}</h1>"
                # Add an unordered list for the subkeys
                html += "<ul>"
                # Iterate over the subkeys and values in the current value dictionary
                for subkey, subvalue in value.items():
                    # If the subvalue is a dictionary, add a link to it in the list
                    if isinstance(subvalue, dict):
                        html += f"<li><a href='#{prefix}{key}{os.path.sep}{subkey}'>{subkey}</a></li>"
                # Close the unordered list
                html += "</ul>"
            # Recursively generate HTML content for the current value
            html += generate_html(value, depth + 1, f"{prefix}{key}{os.path.sep}")

    # Return the generated HTML
    return html

# Define a function to generate commented code output
def commented_main(summary_output_path="commented_code_output"):
    # Set up logging
    logging.basicConfig(filename="app.log", level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

    # Read in all the summaries from the specified output path
    all_summaries = read_summaries(summary_output_path)

    # Generate HTML content for the summaries
    html_content = generate_html(all_summaries)

    # Combine the HTML content into a complete HTML document
    html_output = f"<html><head><title>Commented Code</title></head><body><h1>Commented Code</h1>{html_content}</body></html>"

    try:
        # Write the HTML output to a file
        with open("commented_code.html", "w", encoding="utf-8") as output_file:
            output_file.write(html_output)
        # Log a success message
        logging.info("Generated commented code summary")
    except Exception as e:
        # Log an error message if there was a problem writing the output file
        logging.error(f"Error writing commented code summary: {e}")

# Run the commented code output generation function if this file is being run as the main script
if __name__ == "__main__":
    commented_main()
```

import os
import logging
from read_summaries import read_summaries


def generate_breadcrumbs(prefix):
    breadcrumb_links = []
    path_parts = prefix[:-1].split(os.path.sep)

    for i, part in enumerate(path_parts):
        link_target = os.path.sep.join(path_parts[:i + 1])
        breadcrumb_links.append(f"<a href='#{link_target}'>{part}</a>")

    return " > ".join(breadcrumb_links)


def generate_html(summaries, depth=0, prefix=""):
    html = ""

    if isinstance(summaries, str):
        html += f"<h3>{prefix[:-4]}</h3>"
        html += f"<pre><code>{summaries}</code></pre>"
    else:
        for key, value in summaries.items():
            if isinstance(value, dict):
                breadcrumbs = generate_breadcrumbs(prefix)
                html += f"<h1 id='{prefix}{key}'>{breadcrumbs} > {key}</h1>"
                html += "<ul>"
                for subkey, subvalue in value.items():
                    if isinstance(subvalue, dict):
                        html += f"<li><a href='#{prefix}{key}{os.path.sep}{subkey}'>{subkey}</a></li>"
                html += "</ul>"
            html += generate_html(value, depth + 1, f"{prefix}{key}{os.path.sep}")

    return html


def commented_main(summary_output_path="commented_code_output"):

    # Set up logging
    logging.basicConfig(filename="app.log", level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

    all_summaries = read_summaries(summary_output_path)

    # Generate HTML content
    html_content = generate_html(all_summaries)

    # Combine HTML content
    html_output = f"<html><head><title>Commented Code</title></head><body><h1>Commented Code</h1>{html_content}</body></html>"

    try:
        with open("commented_code.html", "w", encoding="utf-8") as output_file:
            output_file.write(html_output)
        logging.info("Generated commented code summary")
    except Exception as e:
        logging.error(f"Error writing commented code summary: {e}")


if __name__ == "__main__":
    commented_main()</code></pre><h3>generate_overall_summary_single_HTML</h3><pre><code>```
# Import necessary libraries
import os # for interacting with the operating system
import logging # for logging messages
import openai # for using OpenAI API
import json # for working with JSON data
from read_summaries import read_summaries # custom function to read summaries from files

# Function to generate breadcrumb links for HTML display
def generate_breadcrumbs(prefix):
    breadcrumb_links = [] # initialize an empty list for breadcrumb links
    path_parts = prefix[:-1].split(os.path.sep) # split the prefix by path separator and remove the last character

    # Loop through each part of the path
    for i, part in enumerate(path_parts):
        link_target = os.path.sep.join(path_parts[:i + 1]) # join the path parts up to the current index with the path separator
        breadcrumb_links.append(f"<a href='#{link_target}'>{part}</a>") # add a breadcrumb link for the current path part

    return " > ".join(breadcrumb_links) # join the breadcrumb links with a separator and return as a string

# Function to convert JSON data to HTML format
def json_to_html(json_obj):
    html = "<ul>" # initialize an unordered list for the HTML content
    for key, value in json_obj.items(): # loop through each key-value pair in the JSON object
        html += f"<li><strong>{key}:</strong> " # add a list item with a strong tag for the key
        if isinstance(value, dict): # if the value is another dictionary
            html += json_to_html(value) # recursively call the function to add the nested dictionary
        elif isinstance(value, list): # if the value is a list
            html += "<ul>" # add an unordered list for the list items
            for item in value: # loop through each item in the list
                if isinstance(item, dict): # if the item is another dictionary
                    html += json_to_html(item) # recursively call the function to add the nested dictionary
                else:
                    html += f"<li>{item}</li>" # add the item as a list item
            html += "</ul>" # close the unordered list for the list items
        else:
            html += str(value) # add the value as a string
        html += "</li>" # close the list item
    html += "</ul>" # close the unordered list for the JSON object
    return html # return the HTML content as a string

# Function to generate HTML content for summaries
def generate_html(summaries, depth=0, prefix=""):
    html = "" # initialize an empty string for the HTML content

    if isinstance(summaries, str): # if the summary is a string
        html += f"<h3>{prefix[:-4]}</h3>" # add a header with the prefix (excluding the last 4 characters, which are the path separator)
        try:
            json_obj = json.loads(summaries) # try to parse the summary as JSON data
            html += json_to_html(json_obj) # convert the JSON data to HTML format and add to the HTML content
        except json.JSONDecodeError: # if the summary is not valid JSON
            html += f"<p>{summaries}</p>" # add the summary as a paragraph
    else: # if the summary is a dictionary
        for key, value in summaries.items(): # loop through each key-value pair in the dictionary
            if isinstance(value, dict): # if the value is another dictionary
                breadcrumbs = generate_breadcrumbs(prefix) # generate breadcrumb links for the current path
                html += f"<h1 id='{prefix}{key}'>{breadcrumbs} > {key}</h1>" # add a header with an ID and breadcrumb links
                html += "<ul>" # add an unordered list for the subkeys
                for subkey, subvalue in value.items(): # loop through each subkey-subvalue pair in the nested dictionary
                    if isinstance(subvalue, dict): # if the subvalue is another dictionary
                        html += f"<li><a href='#{prefix}{key}{os.path.sep}{subkey}'>{subkey}</a></li>" # add a list item with a link to the subkey
                html += "</ul>" # close the unordered list for the subkeys
            html += generate_html(value, depth + 1, f"{prefix}{key}{os.path.sep}") # recursively call the function to generate HTML content for nested summaries

    return html # return the HTML content as a string

# Main function to run the program
def main(summary_output_path="summary_output"):

    # Set up logging
    logging.basicConfig(filename="app.log", level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

    all_summaries = read_summaries(summary_output_path) # read all summaries from files in the specified path

    # Generate HTML content
    html_content = generate_html(all_summaries)

    # Combine HTML content
    html_output = f"<html><head><title>Project Summary</title></head><body><h1>Project Summary</h1>{html_content}</body></html>"

    try: # try to write the HTML output to a file
        with open("summary_output.html", "w") as f:
            f.write(html_output)
    except Exception as e: # if an error occurs
        logging.error(str(e)) # log the error message to a file


# Call the main function with default arguments
main()
```
```
# This code block opens a file called "overall_summary.html" in write mode with utf-8 encoding.
# The variable "output_file" is assigned to the opened file.
# The contents of "html_output" are then written to the file.
# The file is automatically closed once the block is exited.
try:
    with open("overall_summary.html", "w", encoding="utf-8") as output_file:
        output_file.write(html_output)
    # A log message is generated indicating that the overall summary has been successfully generated.
    logging.info("Generated overall summary")
# If an exception occurs during the try block, the error message is logged.
except Exception as e:
    logging.error(f"Error writing overall summary: {e}")


# This code block checks if this file is being run as the main program.
# If it is, the "main()" function is called.
if __name__ == "__main__":
    main()
```

import os
import logging
import openai
import json
from read_summaries import read_summaries


def generate_breadcrumbs(prefix):
    breadcrumb_links = []
    path_parts = prefix[:-1].split(os.path.sep)

    for i, part in enumerate(path_parts):
        link_target = os.path.sep.join(path_parts[:i + 1])
        breadcrumb_links.append(f"<a href='#{link_target}'>{part}</a>")

    return " > ".join(breadcrumb_links)


def json_to_html(json_obj):
    html = "<ul>"
    for key, value in json_obj.items():
        html += f"<li><strong>{key}:</strong> "
        if isinstance(value, dict):
            html += json_to_html(value)
        elif isinstance(value, list):
            html += "<ul>"
            for item in value:
                if isinstance(item, dict):
                    html += json_to_html(item)
                else:
                    html += f"<li>{item}</li>"
            html += "</ul>"
        else:
            html += str(value)
        html += "</li>"
    html += "</ul>"
    return html


def generate_html(summaries, depth=0, prefix=""):
    html = ""

    if isinstance(summaries, str):
        html += f"<h3>{prefix[:-4]}</h3>"
        try:
            json_obj = json.loads(summaries)
            html += json_to_html(json_obj)
        except json.JSONDecodeError:
            html += f"<p>{summaries}</p>"
    else:
        for key, value in summaries.items():
            if isinstance(value, dict):
                breadcrumbs = generate_breadcrumbs(prefix)
                html += f"<h1 id='{prefix}{key}'>{breadcrumbs} > {key}</h1>"
                html += "<ul>"
                for subkey, subvalue in value.items():
                    if isinstance(subvalue, dict):
                        html += f"<li><a href='#{prefix}{key}{os.path.sep}{subkey}'>{subkey}</a></li>"
                html += "</ul>"
            html += generate_html(value, depth + 1, f"{prefix}{key}{os.path.sep}")

    return html


def main(summary_output_path="summary_output"):

    # Set up logging
    logging.basicConfig(filename="app.log", level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

    all_summaries = read_summaries(summary_output_path)

    # Generate HTML content
    html_content = generate_html(all_summaries)

    # Combine HTML content
    html_output = f"<html><head><title>Project Summary</title></head><body><h1>Project Summary</h1>{html_content}</body></html>"

    try:
        with open("overall_summary.html", "w", encoding="utf-8") as output_file:
            output_file.write(html_output)
        logging.info("Generated overall summary")
    except Exception as e:
        logging.error(f"Error writing overall summary: {e}")


if __name__ == "__main__":
    main()
</code></pre><h3>generate_summaries</h3><pre><code># Import necessary libraries
import os # To interact with the operating system
import logging # To log messages
import openai # To access OpenAI's GPT-3 API
import json # To work with JSON data
import re # To work with regular expressions
import traceback # To handle exceptions
from git import Repo # To interact with Git repositories
from transformers import GPT2Tokenizer # To tokenize text using GPT-2 model

# Create a GPT-2 tokenizer object
gpt2_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Define a function to calculate the average number of characters per token in a given text
def calculate_avg_chars_per_token(text, tokenizer, max_tokens=1024, estimated_ratio=2.5):
    # Print input values and tokenizer object
    # print("Text:", text)
    print("Tokenizer:", tokenizer)
    print("Max tokens:", max_tokens)
    print("Estimated ratio:", estimated_ratio)

    # Tokenize the input text
    tokens = tokenizer.tokenize(text)

    # Calculate the average number of characters per token
    if len(tokens) == 0:
        print("Warning: No tokens found in the text. Returning default value.")
        return estimated_ratio  # return the estimated_ratio or another default value

    avg_chars_per_token = len(text) / len(tokens)

    # Print the total number of characters and tokens in the input text
    print(f"Total characters: {len(text)}, Total tokens: {len(tokens)}")

    # Reduce the calculated average by 25% to provide some allowance
    reduced_avg_chars_per_token = avg_chars_per_token * 0.75

    return reduced_avg_chars_per_token

# Define a function to check if a given file name contains a test keyword
def contains_test_keyword(name):
    test_keywords = {"test", "tests"}
    return any(keyword.lower() in name.lower() for keyword in test_keywords)

# Define a function to get a list of Python files in a given repository path
def get_python_files(repo_path):
    python_files = []
    for root, _, files in os.walk(repo_path):
        for file in files:
            if file.endswith(".py") and not contains_test_keyword(file):
                file_path = os.path.join(root, file)
                print(f"Found Python file: {file_path}")
                python_files.append(file_path)
                print(f"Appending file {file_path} to the list")
    return python_files

# Define a function to clone a Git repository to a local path
def clone_repo(repo_url, local_path):
    try:
        Repo.clone_from(repo_url, local_path)
        logging.info(f"Cloned repository: {repo_url} to {local_path}")
        print(f"Repo cloned successfully: {repo_url} to {local_path}")
        print(f"Cloned repository: {repo_url} to {local_path}")
    except Exception as e:
        logging.error(f"Error cloning repository: {e}")
        print(
            f"Error cloning repository: {repo_url} to {local_path}. Error: {e}"
        )

# Define a function to process the Python files in a given repository path
def process_files(repo_path, output_folder, tokenizer):
    print("Entering process_files function")
    python_files = get_python_files(repo_path)

    # Log the number of Python files found
    logging.info(f"Found {len(python_files)} Python files to process.")
    print(f"Found {len(python_files)} Python files to process.")

    # Process each Python file
    for index, file_path in enumerate(python_files, 1):
        # Log the current file being processed
        logging.info(f"Processing file {index}/{len(python_files)}: {file_path}")
        print(f"Processing file {index}/{len(python_files)}: {file_path}")

        try:
            # Read the content of the file
            content = read_file_content(file_path)

            # Process the content of the file
            print("Processing file content...")
            summaries = process_chunks(content, tokenizer)
            print("Processing file content complete.")

            if len(summaries) == 1:
                # If only one summary was generated, write it to a file
                summary = summaries[0]
                output_file_path = os.path.join(output_folder, os.path.basename(file_path))
                write_file_content(output_file_path, summary)
                print(f"Summary written to file: {output_file_path}")
            else:
                # If multiple summaries were generated, write them to separate files
                for i, summary in enumerate(summaries, 1):
                    output_file_path = os.path.join(output_folder, f"{os.path.basename(file_path)}_{i}.txt")
                    write_file_content(output_file_path, summary)
                    print(f"Summary written to file: {output_file_path}")

        except Exception as e:
            # Log any exceptions that occur during processing
            logging.error(f"Error processing file {file_path}: {e}")
            print(f"Error processing file {file_path}: {e}")
            traceback.print_exc()

# Define a main function to execute the program
def main():
    # Set up logging
    logging.basicConfig(filename="summary.log", level=logging.INFO)

    # Clone the Git repository to a local path
    repo_url = "https://github.com/openai/gpt-3"
    local_path = "./gpt-3"
    clone_repo(repo_url, local_path)

    # Process the Python files in the cloned repository
    repo_path = local_path
    output_folder = "./summaries"
    process_files(repo_path, output_folder, gpt2_tokenizer)

# Call the main function to execute the program
if __name__ == "__main__":
    main()
# This function generates summaries for files in a GitHub repository
def generate_summaries(repo_url, local_path, api_key, output_folder):
    print("Entering generate_summaries function")

    # Set up your OpenAI API key
    openai.api_key = api_key

    # Set up logging
    logging.basicConfig(
        filename="app.log",
        level=logging.INFO,
        format="%(asctime)s %(levelname)s: %(message)s",
    )

    # Clone the GitHub repo to a local directory
    clone_repo(repo_url, local_path)

    # Generate summaries for the files
    print("Starting to process files...")
    process_files(local_path, output_folder, gpt2_tokenizer)
    print("Finished processing files")


# This function chunks the file content into smaller pieces
def chunk_content(file_content, max_tokens):
    tokens = gpt2_tokenizer.encode(file_content)
    chunks = []
    for i in range(0, len(tokens), max_tokens):
        chunk = gpt2_tokenizer.decode(tokens[i:i + max_tokens])
        chunks.append(chunk)
    return chunks


# This function gets a sample of the file content
def get_sample(content, sample_size):
    content_length = len(content)
    if content_length <= sample_size:
        return content

    start_index = (content_length // 2) - (sample_size // 2)
    end_index = start_index + sample_size
    return content[start_index:end_index]


# This function validates the summary
def validate_summary(summary):
    def is_empty_string(s):
        return isinstance(s, str) and s.strip() == ""

    print(f"Received summary: {summary}")
    valid_keys = [
        "Overall Summary",
        "Module/Library Name",
        "Classes",
        "Functions/Methods",
        "Data Structures",
        "Interfaces/APIs",
        "Configuration/Environment Variables",
        "Error Handling/Logging",
        "Data Inputs",
        "Data Outputs"
    ]

    invalid_keys = [key for key in summary if key not in valid_keys]
    if invalid_keys:
        print(f"Invalid keys in summary: {invalid_keys}")
        return False

    invalid_types = []
    for key, value in summary.items():
        if is_empty_string(value):
            invalid_types.append(key)
    if invalid_types:
        print(f"Invalid types in summary: {invalid_types}")
        return False

    return True


# This function processes the files in a local directory and generates summaries for them
def process_files(local_path, output_folder, tokenizer):
    for root, dirs, files in os.walk(local_path):
        for file in files:
            if file.endswith(".py"):
                file_path = os.path.join(root, file)
                print(f"Processing file: {file_path}")
                with open(file_path, "r") as f:
                    file_content = f.read()

                # Chunk the file content into smaller pieces
                chunks = chunk_content(file_content, MAX_TOKENS)

                # Generate summaries for each chunk
                summaries = []
                for chunk in chunks:
                    summary = generate_summary(chunk)
                    summaries.append(summary)

                # Reconstruct the summaries if there is more than one
                if len(summaries) == 1:
                    print("Only one chunk summary generated. Using it as the output.")
                    reconstructed_summary = summaries[0]
                else:
                    print("Reconstructing summaries...")
                    try:
                        reconstructed_summary = reconstruct_summaries(summaries)
                    except Exception as e:
                        print("Error occurred in reconstruct_summaries function:")
                        print(traceback.format_exc())
                        raise e
                    print("Reconstruction complete.")

                # Validate the summary
                if not validate_summary(reconstructed_summary):
                    print("Invalid summary. Skipping.")
                    continue

                # Save the summary to a file
                output_file_path = save_summary(output_folder, repo_path, file_path, json.dumps(reconstructed_summary, indent=2))

                # Log the summary generation and save
                logging.info(f"Generated summary for {file_path} and saved to {output_file_path}")
                print(f"Generated summary for {file_path} and saved to {output_file_path}")
                print(f"Value written to the file: {json.dumps(reconstructed_summary, indent=2)}")

            else:
                print(f"Skipping non-Python file: {file}")


# This function generates a summary for a given text
def generate_summary(text):
    prompt = f"Please summarize the following Python code:\n\n{text}\n\nSummary:"
    response = openai.Completion.create(
        engine="davinci-codex",
        prompt=prompt,
        temperature=0.5,
        max_tokens=MAX_TOKENS,
        n=1,
        stop=None,
        timeout=10,
    )
    summary = response.choices[0].text.strip()
    return summary


# This function reconstructs summaries for multiple chunks of a file
def reconstruct_summaries(summaries):
    reconstructed_summary = {
        "Overall Summary": "",
        "Module/Library Name": "",
        "Classes": "",
        "Functions/Methods": "",
        "Data Structures": "",
        "Interfaces/APIs": "",
        "Configuration/Environment Variables": "",
        "Error Handling/Logging": "",
        "Data Inputs": "",
        "Data Outputs": ""
    }

    for summary in summaries:
        for key, value in summary.items():
            if value:
                if not reconstructed_summary[key]:
                    reconstructed_summary[key] = value
                else:
                    reconstructed_summary[key] += f"\n{value}"

    return reconstructed_summary


# This function saves the summary to a file
def save_summary(output_folder, repo_path, file_path, summary):
    relative_path = os.path.relpath(file_path, repo_path)
    output_file_path = os.path.join(output_folder, relative_path + ".summary")
    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)
    with open(output_file_path, "w") as f:
        f.write(summary)
    return output_file_path
# This function checks if the value of a key in a dictionary is of the correct type or an empty string
# If it is not of the correct type or an empty string, the key is added to a list of invalid types
# If there are any invalid types, the function returns False and prints a message indicating which keys have invalid types
# If all types are valid, the function returns True and prints a message indicating that the summary is valid
def validate_summary(summary):
    invalid_types = []
    # iterate through each key-value pair in the summary dictionary
    for key, value in summary.items():
        # check if the value is not a dictionary or a string
        if not (isinstance(value, dict) or isinstance(value, str)):
            invalid_types.append(key)
        # check if the value is an empty string
        elif is_empty_string(value):
            continue
        # check if the value is a string and is an empty string
        elif isinstance(value, str) and is_empty_string(value):
            invalid_types.append(key)

    # if there are any invalid types, print a message and return False
    if invalid_types:
        print(f"Invalid types in summary: {invalid_types}")
        return False

    # if all types are valid, print a message and return True
    print("Summary is valid")
    return True


# This function returns an empty JSON structure with the required keys and initial values
def empty_json_structure():
    return {
        "Overall Summary": "",
        "Module/Library Name": "",
        "Classes": {},
        "Functions/Methods": {},
        "Data Structures": "",
        "Interfaces/APIs": "",
        "Configuration/Environment Variables": {},
        "Error Handling/Logging": "",
        "Data Inputs": "",
        "Data Outputs": ""
    }


# This function checks if a response is incomplete by checking if it has a closing curly brace and equal number of opening and closing curly braces
def is_response_incomplete(response: str) -> bool:
    print("Calling is_response_incomplete() function...")
    print(f"Analyzing response: {response}")

    # Remove newline characters and white spaces outside of string values
    response = re.sub(r'(?<=\})\s+|\s+(?=\{)', '', response)

    # if response is empty, return False
    if not response:
        return False

    # Check if the response is missing a closing curly brace
    if response[-1] != '}':
        return True

    # Check if the response has an equal number of opening and closing curly braces
    if response.count('{') != response.count('}'):
        return True

    return False


# This function generates a summary of a chunk of text by prompting the user to follow a specific JSON structure
def generate_summary(chunk_text):
    required_keys = [
        "Overall Summary",
        "Module/Library Name",
        "Classes",
        "Functions/Methods",
        "Data Structures",
        "Interfaces/APIs",
        "Configuration/Environment Variables",
        "Error Handling/Logging",
        "Data Inputs",
        "Data Outputs"
    ]

    # Check if the chunk is empty or too small to generate a summary
    if len(chunk_text.strip()) == 0 or len(chunk_text) < 10:
        print("Chunk content is empty or too small to generate a meaningful summary.")
        # create an empty summary with the required keys and initial values
        empty_summary = {key: {} if key in {"Classes", "Functions/Methods", "Configuration/Environment Variables"} else "" for key in required_keys}
        return empty_summary

    prompt = (
        "Generate a JSON structure summary for the content below. "
        "Follow the structure outlined here:\n\n"
        'Example JSON Structure:\n'
        '{\n'
        '  "Overall Summary": "A brief summary of the code snippet...",\n'
        '  "Module/Library Name": "ExampleLib",\n'
        '  "Classes": {"ExampleClass": "A class that..."},\n'
        '  "Functions/Methods": {"example_function": "A function that..."},\n'
        '  "Data Structures": "Lists, dictionaries...",\n'
        '  "Interfaces/APIs": "REST API endpoints...",\n'
        '  "Configuration/Environment Variables": {"EXAMPLE_VAR": "A variable that..."},\n'
        '  "Error Handling/Logging": "Error handling details...",\n'
        '  "Data Inputs": "Description of data inputs...",\n'
        '  "Data Outputs": "Description of data outputs..."\n'
        '}\n\n'
        'Your summary:'
    )

    # prompt the user to input a summary following the required JSON structure
    summary = json.loads(input(prompt))

    # validate the summary and prompt the user to input a new summary if it is invalid
    while not validate_summary(summary):
        summary = json.loads(input(prompt))

    return summary
This code is a script that uses the OpenAI API to generate a response in JSON format to a prompt provided by the user. The prompt should ask for a summary of a code snippet and provide a chunk of text to be included in the response.

The script first defines a dictionary with keys for various aspects of code documentation, such as overall summary, classes, functions/methods, and data inputs/outputs. It then formats this dictionary as a JSON string and includes it in the prompt message to be sent to the API.

The script then enters a loop that will attempt to make an API call to OpenAI's chat completion model. The loop will retry up to 5 times if the API call fails. The API call includes the prompt message and some parameters for the chat completion model, such as the maximum number of tokens to generate and the temperature to use for sampling from the model's output distribution.

If the API call is successful, the script prints a message indicating success. If the API call fails, the script prints an error message indicating the reason for the failure.
# The function below tries to generate a summary for the input text using an API call and then validates the generated summary.
# If the generated summary is valid, it is returned as a dictionary. If not, the function retries up to 5 times with increasing max_tokens.
# If the function is unable to generate a valid summary after 5 attempts, it returns an empty JSON structure.

import re
import json

def generate_summary(text):
    retries = 5
    max_tokens = 200
    headers = {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer API_KEY'
    }

    while retries > 0:
        data = {
            'text': text,
            'model': 'text-2-api',
            'max_tokens': max_tokens,
            'temperature': 0.7,
            'stop': ['\n']
        }

        try:
            response = requests.post(API_URL, headers=headers, json=data).json()
        except Exception as e:
            raise Exception(f"API call error: {e}")

        # Extract the summary from the API response
        summary = response['choices'][0]['message']['content'].strip()
        print(f"Attempt {6 - retries} summary:\n{summary}")

        # Check if the summary contains a JSON object
        json_pattern = re.compile(r'(?s)\{.*}')
        json_match = json_pattern.search(summary)
        if json_match:
            # Extract the JSON object from the summary and print it
            json_text = json_match.group()
            print("Matched JSON text:")
            print(json_text)
        else:
            print("No JSON text found in the summary.")
            json_text = ""

        # Load the JSON object into a dictionary and print it
        try:
            summary_dict = json.loads(json_text)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e}")
            summary_dict = {}
        print("JSON dictionary after loading:")
        print(summary_dict)

        # Validate the summary and return it if it is valid
        if summary_dict:
            print("Sending the following JSON dictionary to the validate_summary function:")
            print(summary_dict)
            validation_result = validate_summary(summary_dict)
            if validation_result is True:
                print("Validation result: True")
                return summary_dict
            else:
                # If the summary is invalid, check if it is incomplete and increase max_tokens if it is
                is_response_incomplete_result = is_response_incomplete(summary)
                print(f"[4.2] is_response_incomplete() returned: {is_response_incomplete_result}")
                if is_response_incomplete_result:
                    print("[4.1] Incomplete response detected, increasing max_tokens...")
                    max_tokens += 100
                else:
                    print("[4.3] Summary validation failed, retrying...")
                retries -= 1
        else:
            print("[3] Empty summary received, retrying...")
            retries -= 1

    print("[5] Failed to generate a valid summary after 5 attempts. Returning empty JSON structure.")
    return empty_json_structure()


# The function below takes a dictionary of summaries and processes the values to remove any leading/trailing whitespace.
# The processed dictionary is returned as a new dictionary.

def reconstruct_summaries(summaries):
    def process_value(value):
        if isinstance(value, str):
            return value.strip()
        elif isinstance(value, dict):
            return {k: process_value(v) for k, v in value.items()}
        elif isinstance(value, list):
            return [process_value(v) for v in value]
        return value

    # Define the structure of the reconstructed summary
    reconstructed_summary = {
        "Overall Summary": "",
        "Module/Library Name": "",
        "Classes": {},
        "Functions/Methods": {},
        "Data Structures": "",
        "Interfaces/APIs": "",
        "Configuration/Environment Variables": {},
        "Error Handling/Logging": "",
        "Data Inputs": "",
        "Data Outputs": ""
    }

    # Process the values in the input dictionary and store them in the reconstructed summary
    for k, v in summaries.items():
        reconstructed_summary[k] = process_value(v)

    print(f"Input summaries: {summaries}")
    print(f"Reconstructed summary: {reconstructed_summary}")
    return reconstructed_summary
```
# This function takes a string and a tokenizer as input and returns the average number of characters per token
def calculate_avg_chars_per_token(content, tokenizer):
    tokens = tokenizer.tokenize(content)
    total_chars = sum(len(token) for token in tokens)
    avg_chars_per_token = total_chars / len(tokens)
    return avg_chars_per_token


# This function takes a list of summaries as input, deduplicates them, and removes trailing commas and spaces
def reconstruct_summary(summaries):
    reconstructed_summary = {
        "Overall Summary": "",
        "Module/Library Name": "",
        "Data Inputs": "",
        "Data Outputs": "",
        "Data Structures": "",
        "Interfaces/APIs": "",
        "Error Handling/Logging": "",
        "Classes": {},
        "Functions/Methods": {},
        "Configuration/Environment Variables": {}
    }

    # Iterate through each summary and process its values
    for summary in summaries:
        summary = process_value(summary)
        if not isinstance(summary, dict):
            print(f"Error: summary is not a dictionary: {summary}")
            continue

        # Iterate through each key in the summary and add its values to the reconstructed summary
        for key in summary:
            if key in reconstructed_summary:
                # If the key is one of the standard summary categories, add its value to the reconstructed summary
                if key in {"Overall Summary", "Module/Library Name", "Data Inputs", "Data Outputs", "Data Structures", "Interfaces/APIs", "Error Handling/Logging"}:
                    if summary[key] and (isinstance(summary[key], str) and summary[key].lower() not in {"none", "n/a", ""}):
                        if not reconstructed_summary[key]:
                            reconstructed_summary[key] = summary[key]
                        else:
                            reconstructed_summary[key] += f", {summary[key]}"
                # If the key is one of the custom summary categories, add its values to the appropriate dictionary
                elif key in {"Classes", "Functions/Methods", "Configuration/Environment Variables"}:
                    if isinstance(summary[key], dict):
                        for item_key, item_value in summary[key].items():
                            if item_value.lower() not in {"none", "n/a", ""}:
                                if item_key not in reconstructed_summary[key]:
                                    reconstructed_summary[key][item_key] = item_value
                            else:
                                print(f"Skipped {item_key} with value {item_value} for key {key}")
                    else:
                        print(f"Error at summary: {summary}, key: {key}")

    # Deduplicate and remove trailing commas and spaces from the standard summary categories
    for key in {"Overall Summary", "Module/Library Name", "Data Inputs", "Data Outputs", "Data Structures", "Interfaces/APIs", "Error Handling/Logging"}:
        values = reconstructed_summary[key].split(', ')
        deduplicated_values = list(dict.fromkeys(values))
        reconstructed_summary[key] = ', '.join(deduplicated_values).rstrip(", ")

    return reconstructed_summary


# This function reads the content of a file and returns it as a string
def read_file_content(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
    return content


# This function takes a string and a tokenizer as input, splits the string into chunks based on the maximum number of characters per chunk, and returns a list of chunks
def process_chunks(content, tokenizer):
    # Extract a sample from the content
    sample_size = 2500
    content_sample = content[:sample_size]

    # Calculate the average characters per token for the sample
    avg_chars_per_token = calculate_avg_chars_per_token(content_sample, tokenizer)

    # Reserve 950 tokens for the model's response and calculate the maximum number of characters per chunk
    max_chunk_size = 4096 - 1050
    max_chars_per_chunk = int(max_chunk_size * avg_chars_per_token)

    # Split the content into chunks based on the maximum number of characters per chunk
    chunks = []
    current_chunk = ""
    for line in content.splitlines():
        if len(current_chunk) + len(line) + 1 <= max_chars_per_chunk:
            current_chunk += f"{line}\n"
        else:
            chunks.append(current_chunk.strip())
            current_chunk = f"{line}\n"
    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks


# This function takes a value and returns a processed version of the value
def process_value(value):
    if isinstance(value, str):
        return value.strip()
    elif isinstance(value, list):
        processed_list = []
        for item in value:
            processed_item = process_value(item)
            if processed_item:
                processed_list.append(processed_item)
        return processed_list
    elif isinstance(value, dict):
        processed_dict = {}
        for key, item in value.items():
            processed_key = process_value(key)
            processed_item = process_value(item)
            if processed_key and processed_item:
                processed_dict[processed_key] = processed_item
        return processed_dict
    else:
        return value
```
# This function takes in a string of content and splits it into chunks of a specified maximum length, while also ensuring that each chunk ends at a word or paragraph boundary
def split_content_into_chunks(content: str, max_chars_per_chunk: int) -> List[str]:
    # Initialize an empty list to store the content chunks
    content_chunks = []
    # Initialize a variable to keep track of the starting index of each chunk
    start_idx = 0

    # Loop through the content until all chunks have been created
    while start_idx < len(content):
        # Calculate the end index for the current chunk
        end_idx = start_idx + max_chars_per_chunk

        # Find the closest word or paragraph boundary (whitespace or newline character) to end the chunk at
        while end_idx < len(content) and content[end_idx] not in {'\n', ' '}:
            end_idx -= 1

        # Extract the current chunk and add it to the list of content chunks
        chunk = content[start_idx:end_idx]
        content_chunks.append(chunk)
        # Update the starting index for the next chunk
        start_idx = end_idx

    # Print the number of chunks created
    print(f"Number of chunks: {len(content_chunks)}")

    # Initialize an empty list to store the summaries for each chunk
    summaries = []
    # Loop through each content chunk and generate a summary for it
    for chunk in content_chunks:
        print("Processing chunk for summary generation...")
        summary = generate_summary(chunk)
        print("Summary generation complete for this chunk.")
        summaries.append(summary)

    # Return the list of summaries for each content chunk
    return summaries


# This function saves a summary to a file in a specified output folder, with a filename based on the original file path
def save_summary(output_folder: str, repo_path: str, file_path: str, summary: str) -> str:
    # Get the relative path of the file with respect to the local repository path
    relative_path = os.path.relpath(file_path, repo_path)

    # Replace the .py extension with .txt for the summary file
    summary_filename = f"{os.path.splitext(relative_path)[0]}.txt"

    # Create the corresponding folder structure inside the output folder
    output_file_path = os.path.join(output_folder, summary_filename)
    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)

    # Write the summary to the output file
    with open(output_file_path, 'w', encoding='utf-8') as summary_file:
        summary_file.write(summary)

    # Print the path where the summary was saved
    print(f"Summary for {file_path} saved at {output_file_path}")
    # Return the path where the summary was saved
    return output_file_path

import os
import logging
import openai
import json
import re
import traceback
from git import Repo
from transformers import GPT2Tokenizer

gpt2_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")


def calculate_avg_chars_per_token(text, tokenizer, max_tokens=1024, estimated_ratio=2.5):
    # Print input values and tokenizer object
    # print("Text:", text)
    print("Tokenizer:", tokenizer)
    print("Max tokens:", max_tokens)
    print("Estimated ratio:", estimated_ratio)

    # Tokenize the input text
    tokens = tokenizer.tokenize(text)

    # Calculate the average number of characters per token
    if len(tokens) == 0:
        print("Warning: No tokens found in the text. Returning default value.")
        return estimated_ratio  # return the estimated_ratio or another default value

    avg_chars_per_token = len(text) / len(tokens)

    print(f"Total characters: {len(text)}, Total tokens: {len(tokens)}")  # Add this print statement

    # Reduce the calculated average by 25% to provide some allowance
    reduced_avg_chars_per_token = avg_chars_per_token * 0.75

    return reduced_avg_chars_per_token


def contains_test_keyword(name):
    test_keywords = {"test", "tests"}
    return any(keyword.lower() in name.lower() for keyword in test_keywords)


def get_python_files(repo_path):
    python_files = []
    for root, _, files in os.walk(repo_path):
        for file in files:
            if file.endswith(".py") and not contains_test_keyword(file):
                file_path = os.path.join(root, file)
                print(f"Found Python file: {file_path}")
                python_files.append(file_path)
                print(f"Appending file {file_path} to the list")
    return python_files


def clone_repo(repo_url, local_path):
    try:
        Repo.clone_from(repo_url, local_path)
        logging.info(f"Cloned repository: {repo_url} to {local_path}")
        print(f"Repo cloned successfully: {repo_url} to {local_path}")
        print(f"Cloned repository: {repo_url} to {local_path}")
    except Exception as e:
        logging.error(f"Error cloning repository: {e}")
        print(
            f"Error cloning repository: {repo_url} to {local_path}. Error: {e}"
        )


def process_files(repo_path, output_folder, tokenizer):
    print("Entering process_files function")
    python_files = get_python_files(repo_path)

    logging.info(f"Found {len(python_files)} Python files to process.")
    print(f"Found {len(python_files)} Python files to process.")

    for index, file_path in enumerate(python_files, 1):
        logging.info(f"Processing file {index}/{len(python_files)}: {file_path}")
        print(f"Processing file {index}/{len(python_files)}: {file_path}")

        try:
            content = read_file_content(file_path)

            print("Processing file content...")
            summaries = process_chunks(content, tokenizer)
            print("Processing file content complete.")

            if len(summaries) == 1:
                print("Only one chunk summary generated. Using it as the output.")
                reconstructed_summary = summaries[0]
            else:
                print("Reconstructing summaries...")
                try:
                    reconstructed_summary = reconstruct_summaries(summaries)
                except Exception as e:
                    print("Error occurred in reconstruct_summaries function:")
                    print(traceback.format_exc())
                    raise e
                print("Reconstruction complete.")

            print("Reconstructed summary:\n", json.dumps(reconstructed_summary, indent=2))

            output_file_path = save_summary(output_folder, repo_path, file_path, json.dumps(reconstructed_summary, indent=2))

            logging.info(f"Generated summary for {file_path} and saved to {output_file_path}")
            print(f"Generated summary for {file_path} and saved to {output_file_path}")
            print(f"Value written to the file: {json.dumps(reconstructed_summary, indent=2)}")

        except Exception as e:
            logging.error(f"Error processing file {file_path}: {e}")


def generate_summaries(repo_url, local_path, api_key, output_folder):
    print("Entering generate_summaries function")

    # Set up your OpenAI API key
    openai.api_key = api_key

    # Set up logging
    logging.basicConfig(
        filename="app.log",
        level=logging.INFO,
        format="%(asctime)s %(levelname)s: %(message)s",
    )

    # Clone the GitHub repo to a local directory
    clone_repo(repo_url, local_path)

    # Generate summaries for the files
    print("Starting to process files...")
    process_files(local_path, output_folder, gpt2_tokenizer)
    print("Finished processing files")


def chunk_content(file_content, max_tokens):
    tokens = gpt2_tokenizer.encode(file_content)
    chunks = []
    for i in range(0, len(tokens), max_tokens):
        chunk = gpt2_tokenizer.decode(tokens[i:i + max_tokens])
        chunks.append(chunk)
    return chunks


def get_sample(content, sample_size):
    content_length = len(content)
    if content_length <= sample_size:
        return content

    start_index = (content_length // 2) - (sample_size // 2)
    end_index = start_index + sample_size
    return content[start_index:end_index]


def validate_summary(summary):
    def is_empty_string(s):
        return isinstance(s, str) and s.strip() == ""

    print(f"Received summary: {summary}")
    valid_keys = [
        "Overall Summary",
        "Module/Library Name",
        "Classes",
        "Functions/Methods",
        "Data Structures",
        "Interfaces/APIs",
        "Configuration/Environment Variables",
        "Error Handling/Logging",
        "Data Inputs",
        "Data Outputs"
    ]

    invalid_keys = [key for key in summary if key not in valid_keys]
    if invalid_keys:
        print(f"Invalid keys in summary: {invalid_keys}")
        return False

    invalid_types = []
    for key, value in summary.items():
        if not (isinstance(value, dict) or isinstance(value, str)):
            invalid_types.append(key)
        elif is_empty_string(value):
            continue
        elif isinstance(value, str) and is_empty_string(value):
            invalid_types.append(key)

    if invalid_types:
        print(f"Invalid types in summary: {invalid_types}")
        return False

    print("Summary is valid")
    return True


def empty_json_structure():
    return {
        "Overall Summary": "",
        "Module/Library Name": "",
        "Classes": {},
        "Functions/Methods": {},
        "Data Structures": "",
        "Interfaces/APIs": "",
        "Configuration/Environment Variables": {},
        "Error Handling/Logging": "",
        "Data Inputs": "",
        "Data Outputs": ""
    }


def is_response_incomplete(response: str) -> bool:
    print("Calling is_response_incomplete() function...")
    print(f"Analyzing response: {response}")

    # Remove newline characters and white spaces outside of string values
    response = re.sub(r'(?<=\})\s+|\s+(?=\{)', '', response)

    if not response:
        return False

    # Check if the response is missing a closing curly brace
    if response[-1] != '}':
        return True

    # Check if the response has an equal number of opening and closing curly braces
    if response.count('{') != response.count('}'):
        return True

    return False


def generate_summary(chunk_text):
    required_keys = [
        "Overall Summary",
        "Module/Library Name",
        "Classes",
        "Functions/Methods",
        "Data Structures",
        "Interfaces/APIs",
        "Configuration/Environment Variables",
        "Error Handling/Logging",
        "Data Inputs",
        "Data Outputs"
    ]

    # Check if the chunk is empty or too small to generate a summary
    if len(chunk_text.strip()) == 0 or len(chunk_text) < 10:  # Adjust the threshold as needed
        print("Chunk content is empty or too small to generate a meaningful summary.")
        empty_summary = {key: {} if key in {"Classes", "Functions/Methods", "Configuration/Environment Variables"} else "" for key in required_keys}
        return empty_summary

    prompt = (
        "Generate a JSON structure summary for the content below. "
        "Follow the structure outlined here:\n\n"
        'Example JSON Structure:\n'
        '{\n'
        '  "Overall Summary": "A brief summary of the code snippet...",\n'
        '  "Module/Library Name": "ExampleLib",\n'
        '  "Classes": {"ExampleClass": "A class that..."},\n'
        '  "Functions/Methods": {"example_function": "A function that..."},\n'
        '  "Data Structures": "Lists, dictionaries...",\n'
        '  "Interfaces/APIs": "REST API endpoints...",\n'
        '  "Configuration/Environment Variables": {"EXAMPLE_VAR": "A variable that..."},\n'
        '  "Error Handling/Logging": "Error handling details...",\n'
        '  "Data Inputs": "Description of the data inputs...",\n'
        '  "Data Outputs": "Description of the data outputs..."\n'
        '}\n\n'
        "1. Overall Summary: Provide a brief summary of the code snippet.\n"
        "2. Module/Library Name: Provide the name of the module or library used in the file.\n"
        "3. Classes: Briefly describe the purpose, properties, and methods for each class in the file, "
        "including inheritance and composition relationships, if applicable.\n"
        "4. Functions/Methods: Briefly describe each function or method in the file, "
        "including input parameters, return values, and any side effects. "
        "Document any significant algorithms or logic used within the functions.\n"
        "5. Data Structures: Describe the key data structures used in the file, "
        "such as lists, dictionaries, or custom data structures, and their purposes. "
        "Include any significant relationships or interactions between these structures.\n"
        "6. Interfaces/APIs: If the file exposes any APIs or interfaces for integration with other systems, "
        "document their endpoints, input and output formats, and any authentication or authorization requirements.\n"
        "7. Configuration/Environment Variables: List any configuration files or environment variables "
        "required for the file, and describe their purposes and possible values.\n"
        "8. Error Handling/Logging: Explain the error handling and logging mechanisms used in the file, "
        "including any specific error codes or messages that developers should be aware of.\n"
        "9. Data Inputs: Describe the data inputs for the file, including any file formats, "
        "data sources, or user input requirements.\n"
        "10. Data Outputs: Describe the data outputs for the file, including any file formats, "
        "data destinations, or user output requirements.\n\n"
        f"{chunk_text}\n"
        "Important: Summarize the content without using any part of the original code. "
        "Ensure your response is a useful summary for a developer to read, "
        "and always provide a JSON structure with all keys present in the example."
    )

    retries = 5
    max_tokens = 900
    while retries > 0:
        print(f"Retries left: {retries}")

        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "You are a helpful assistant that only supplies responses in JSON structures."
                        ),
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=max_tokens,
                n=1,
                temperature=0.5,
            )
            print("[2] API call successful.")
        except Exception as e:
            print("[2] Error during API call:", e)
            raise Exception(f"API call error: {e}")

        summary = response['choices'][0]['message']['content'].strip()
        print(f"Attempt {6 - retries} summary:\n{summary}")

        json_pattern = re.compile(r'(?s)\{.*}')
        json_match = json_pattern.search(summary)
        if json_match:
            json_text = json_match.group()
            print("Matched JSON text:")  # New print statement
            print(json_text)  # New print statement
        else:
            print("No JSON text found in the summary.")
            json_text = ""

        try:
            summary_dict = json.loads(json_text)  # Changed from 'summary' to 'json_text'
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e}")
            summary_dict = {}

        print("JSON dictionary after loading:")  # New print statement
        print(summary_dict)  # New print statement

        if summary_dict:
            print("Sending the following JSON dictionary to the validate_summary function:")
            print(summary_dict)
            validation_result = validate_summary(summary_dict)
            if validation_result is True:
                print("Validation result: True")
                return summary_dict
            else:
                print("Validation result: False")
                is_response_incomplete_result = is_response_incomplete(
                    summary)  # Store the result of is_response_incomplete() in a variable
                print(
                    f"[4.2] is_response_incomplete() returned: {is_response_incomplete_result}")  # Print the result regardless of its value
                if is_response_incomplete_result:
                    print("[4.1] Incomplete response detected, increasing max_tokens...")
                    max_tokens += 100
                else:
                    print("[4.3] Summary validation failed, retrying...")

                retries -= 1  # Move the retries decrement outside of the else block
        else:
            print("[3] Empty summary received, retrying...")
            retries -= 1

    print("[5] Failed to generate a valid summary after 5 attempts. Returning empty JSON structure.")
    return empty_json_structure()


def reconstruct_summaries(summaries):
    def process_value(value):
        if isinstance(value, str):
            return value.strip()
        elif isinstance(value, dict):
            return {k: process_value(v) for k, v in value.items()}
        elif isinstance(value, list):
            return [process_value(v) for v in value]
        return value

    reconstructed_summary = {
        "Overall Summary": "",
        "Module/Library Name": "",
        "Classes": {},
        "Functions/Methods": {},
        "Data Structures": "",
        "Interfaces/APIs": "",
        "Configuration/Environment Variables": {},
        "Error Handling/Logging": "",
        "Data Inputs": "",
        "Data Outputs": ""
    }
    print(f"Input summaries: {summaries}")

    for summary in summaries:
        summary = process_value(summary)
        if not isinstance(summary, dict):
            print(f"Error: summary is not a dictionary: {summary}")
            continue

        for key in summary:
            if key in reconstructed_summary:
                if key in {"Overall Summary", "Module/Library Name", "Data Inputs", "Data Outputs", "Data Structures", "Interfaces/APIs", "Error Handling/Logging"}:
                    if summary[key] and (isinstance(summary[key], str) and summary[key].lower() not in {"none", "n/a", ""}):
                        if not reconstructed_summary[key]:
                            reconstructed_summary[key] = summary[key]
                        else:
                            reconstructed_summary[key] += f", {summary[key]}"
                elif key in {"Classes", "Functions/Methods", "Configuration/Environment Variables"}:
                    if isinstance(summary[key], dict):
                        for item_key, item_value in summary[key].items():
                            if item_value.lower() not in {"none", "n/a", ""}:
                                if item_key not in reconstructed_summary[key]:
                                    reconstructed_summary[key][item_key] = item_value
                            else:
                                print(f"Skipped {item_key} with value {item_value} for key {key}")
                    else:
                        print(f"Error at summary: {summary}, key: {key}")

    # Deduplicate and remove trailing commas and spaces
    for key in {"Overall Summary", "Module/Library Name", "Data Inputs", "Data Outputs", "Data Structures", "Interfaces/APIs", "Error Handling/Logging"}:
        values = reconstructed_summary[key].split(', ')
        deduplicated_values = list(dict.fromkeys(values))
        reconstructed_summary[key] = ', '.join(deduplicated_values).rstrip(", ")

    return reconstructed_summary


def read_file_content(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
    return content


def process_chunks(content, tokenizer):
    print("Entered process_chunks function")

    # Extract a sample from the content
    sample_size = 2500  # You can adjust this value
    content_sample = content[:sample_size]
    # print("Extracted content sample:", content_sample)

    # Calculate the average characters per token for the sample
    print("Calling calculate_avg_chars_per_token function")
    avg_chars_per_token = calculate_avg_chars_per_token(content_sample, tokenizer)
    print(f"Average characters per token: {avg_chars_per_token}")

    max_chunk_size = 4096 - 1050  # Reserve 950 tokens for the model's response
    print(f"Max chunk size: {max_chunk_size}")

    # Calculate the maximum number of characters per chunk
    max_chars_per_chunk = int(max_chunk_size * avg_chars_per_token)

    # Split the content into chunks based on the maximum number of characters per chunk
    content_chunks = []
    start_idx = 0

    while start_idx < len(content):
        end_idx = start_idx + max_chars_per_chunk

        # Find the closest word or paragraph boundary (whitespace or newline character)
        while end_idx < len(content) and content[end_idx] not in {'\n', ' '}:
            end_idx -= 1

        chunk = content[start_idx:end_idx]
        content_chunks.append(chunk)
        start_idx = end_idx

    print(f"Number of chunks: {len(content_chunks)}")

    summaries = []
    for chunk in content_chunks:
        print("Processing chunk for summary generation...")
        summary = generate_summary(chunk)
        print("Summary generation complete for this chunk.")
        summaries.append(summary)

    return summaries


def save_summary(output_folder, repo_path, file_path, summary):
    # Get the relative path of the file with respect to the local repository path
    relative_path = os.path.relpath(file_path, repo_path)

    # Replace the .py extension with .txt for the summary file
    summary_filename = f"{os.path.splitext(relative_path)[0]}.txt"

    # Create the corresponding folder structure inside the output folder
    output_file_path = os.path.join(output_folder, summary_filename)
    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)

    with open(output_file_path, 'w', encoding='utf-8') as summary_file:
        summary_file.write(summary)

    print(f"Summary for {file_path} saved at {output_file_path}")
    return output_file_path
</code></pre><h3>main</h3><pre><code># Importing necessary modules
import openai
import os
from generate_summaries import generate_summaries

if __name__ == "__main__":
    # Defining the URL of the repository to be crawled
    repo_url = "https://github.com/department-for-transport-BODS/bods"

    # Defining the base path of the project and local path to store the repository
    base_path = os.path.expanduser(os.path.join("~", "PycharmProjects", "github-crawler"))
    local_path = os.path.join(base_path, "local_repo")
    # Defining the output folder path where summaries will be generated
    output_folder = os.path.join(base_path, "summary_output")

    # Fetching the OpenAI API key from environment variables
    api_key = os.environ.get("OPENAI_API_KEY")
    # Printing the API key for verification purposes
    print(f"API Key: {api_key}")

    # Checking if the API key is present or not
    if not api_key:
        # Raising an error if the API key is not present
        raise ValueError("No OpenAI API key provided. Please set the OPENAI_API_KEY environment variable.")
    # Generating summaries using the generate_summaries function
    generate_summaries(repo_url, local_path, api_key, output_folder)

import openai
import os
from generate_summaries import generate_summaries

if __name__ == "__main__":
    repo_url = "https://github.com/department-for-transport-BODS/bods"

    # Use os.path.join to create platform-independent paths
    base_path = os.path.expanduser(os.path.join("~", "PycharmProjects", "github-crawler"))
    local_path = os.path.join(base_path, "local_repo")
    output_folder = os.path.join(base_path, "summary_output")

    api_key = os.environ.get("OPENAI_API_KEY")
    print(f"API Key: {api_key}")

    if not api_key:
        raise ValueError("No OpenAI API key provided. Please set the OPENAI_API_KEY environment variable.")
    generate_summaries(repo_url, local_path, api_key, output_folder)
</code></pre><h3>read_summaries</h3><pre><code># Import the os module
import os

# Define a function named read_summaries that takes an output_folder as input
def read_summaries(output_folder):
    # Create an empty dictionary named summaries
    summaries = {}

    # Walk through all the directories and files in the output_folder
    for foldername, subfolders, filenames in os.walk(output_folder):
        # Loop through all the files in the current directory
        for filename in filenames:
            # Check if the file has a .txt or .py extension
            if filename.endswith(".txt") or filename.endswith(".py"):
                # Create the full path to the file
                file_path = os.path.join(foldername, filename)
                # Open the file and read its contents
                with open(file_path, "r", encoding="utf-8") as file:
                    summary = file.read()

                # Get the relative path of the file from the output_folder
                relative_path = os.path.relpath(file_path, output_folder)
                # Split the path into its individual parts
                path_parts = relative_path.split(os.path.sep)

                # Traverse the summaries dictionary using the path_parts as keys
                current_level = summaries
                for part in path_parts[:-1]:
                    if part not in current_level:
                        current_level[part] = {}
                    current_level = current_level[part]

                # Add the summary as a value to the last key in the path_parts list
                current_level[path_parts[-1]] = summary

    # Return the summaries dictionary
    return summaries

import os


def read_summaries(output_folder):
    summaries = {}

    for foldername, subfolders, filenames in os.walk(output_folder):
        for filename in filenames:
            if filename.endswith(".txt") or filename.endswith(".py"):
                file_path = os.path.join(foldername, filename)
                with open(file_path, "r", encoding="utf-8") as file:
                    summary = file.read()

                relative_path = os.path.relpath(file_path, output_folder)
                path_parts = relative_path.split(os.path.sep)

                current_level = summaries
                for part in path_parts[:-1]:
                    if part not in current_level:
                        current_level[part] = {}
                    current_level = current_level[part]

                current_level[path_parts[-1]] = summary

    return summaries
</code></pre></body></html>